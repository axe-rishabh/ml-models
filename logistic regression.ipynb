{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741855e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26f6f2",
   "metadata": {},
   "source": [
    "# Linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee5cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets implement lienar combination in vectorized form\n",
    "\n",
    "def linear_combination(X:np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    '''Calculate linear combination of features.\n",
    "    \n",
    "    The linear combination is calculated with the following vectorized form\n",
    "    z = Xw\n",
    "    \n",
    "    Args:\n",
    "        X: feature matrix with shape(n,m)\n",
    "        w: weight vector with shape(m,)\n",
    "        \n",
    "    Returns:\n",
    "        Linear combination of feature with (n,)\n",
    "        \n",
    "    '''\n",
    "    return X@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def1e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's implement sigmoid function in  a vectorized form\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    '''Calculates sigmoid of linear combination of features z.\n",
    "    \n",
    "    Args:\n",
    "        z: list of floats\n",
    "        \n",
    "    Returns:\n",
    "        List of output of sigmoid function\n",
    "    '''\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e6f1e",
   "metadata": {},
   "source": [
    "### if activation or probability > threshold then we label the sample with class 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0252c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X:np.ndarray, w:np.ndarray, threshold: float)-> np.ndarray:\n",
    "    '''Predicts class label for samples.\n",
    "    \n",
    "    The samples are represented with a bunch of features and are presented in form of a freature matrix. The class label is predicted as follows:\n",
    "    * If sigmoid(Xw) > threshold, the sample is labeled with class 1.\n",
    "        * else class 0\n",
    "        \n",
    "    Args:\n",
    "        X: feature vector of shape(n,m)\n",
    "        w: weight vector of shape(m,)\n",
    "        threshold: probability threshold for classification\n",
    "        \n",
    "    Returns:\n",
    "        A list of class labels of shape (n,)\n",
    "    '''\n",
    "    return np.where(sigmoid(linear_combination(X,w)) > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee51e3",
   "metadata": {},
   "source": [
    "## applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6410fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix:  (2, 3)\n",
      "Shape of weight vector:  (3,)\n",
      "Shape of output:  (2,)\n",
      "The class label vector is:  [1 1]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = np.array([[1,20,2], [1,2,2]])\n",
    "weight_vector = np.array([-1,0,1])\n",
    "\n",
    "print('Shape of feature matrix: ', feature_matrix.shape)\n",
    "print('Shape of weight vector: ', weight_vector.shape)\n",
    "\n",
    "class_labels = predict(feature_matrix, weight_vector, 0.5)\n",
    "\n",
    "print('Shape of output: ', class_labels.shape)\n",
    "print('The class label vector is: ', class_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00348f89",
   "metadata": {},
   "source": [
    "# loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081fd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, sigmoid_vectors, weight_vector, l1_reg_rate, l2_reg_rate):\n",
    "    return( -1 * (np.sum(y * np.log(sigmoid_vector) + (1-y) * np.log(1-sigmoid_vector)))\n",
    "          + l2_reg_rate * np.dot(np.transpose(weight_vector), weight_vector)\n",
    "          + l1_reg_rate * np.sum(np.abs(weight_vector)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c6088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X:np.ndarray, y:np.ndarray, w:np.ndarray, reg_rate:float) -> np.ndarray:\n",
    "    '''Calculates gradients of loss function w.r.t. weight vector on training set.\n",
    "    The gradient is claculated with the following vectorized operation:\n",
    "        np.transpose(X)(sigmoid(Xw) -y) + \\lambda w\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix for training data.\n",
    "        y: Label vector for training data.\n",
    "        reg_rate: regularization rate\n",
    "        \n",
    "    Returns:\n",
    "        A vector of gradients.\n",
    "    '''\n",
    "    \n",
    "    return np.transpose(X) @ (sigmoid(linear_combination(X,w)) - y) + reg_rate*w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0236c7",
   "metadata": {},
   "source": [
    "# Logistic regression class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9529ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    '''Logistic regression model.\n",
    "    y = sigmoid(X @ w)\n",
    "    '''\n",
    "    def set_weight_vector(self, w):\n",
    "        self.w = w\n",
    "    \n",
    "    def linear_combination(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''Calculates linear combination of features.\n",
    "        The linear combination is calculated with the following vectorized from:\n",
    "            z = Xw\n",
    "            \n",
    "        Args:\n",
    "            X: feature matrix with shape(n,m)\n",
    "        \n",
    "        Returns:\n",
    "            Linear combination of features with shape(n,)\n",
    "        '''\n",
    "        return X@self.w\n",
    "    \n",
    "    def sigmoid(self, z: np.ndarray):\n",
    "        '''Return probability of input belonging class 1.\n",
    "        Args:\n",
    "            z : (n, ) np.ndarray\n",
    "        Returns:\n",
    "            sigmoid activation vector (n,) np.ndarray\n",
    "        '''\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def activation(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''Calculates sigmoid activation for logistic regression.\n",
    "        \n",
    "        The sigmoid activation is calculated with the following vectorized form:\n",
    "            act = sigmoid(Xw)\n",
    "            \n",
    "        Args:\n",
    "            X: feature matrix with shape (n,m)\n",
    "            \n",
    "        Returns:\n",
    "            activation vector with shape (n,)\n",
    "        '''\n",
    "        return self.sigmoid(self.linear_combination(X))\n",
    "    def predict(self, x: np.ndarray, threshold: float = 0.5):\n",
    "        '''Classify input data.\n",
    "        Args:\n",
    "            x; (N,D) np.ndarray\n",
    "            threshold : float, optional\n",
    "                threshold of binary classification (default is 0.5)\n",
    "        \n",
    "        Returns:\n",
    "            (N, ) np.ndarray\n",
    "        '''\n",
    "        return (self.activation(x) > threshold).astype(int)\n",
    "    def loss(self, X:np.ndarray, y: np.ndarray, reg_rate: float) -> float:\n",
    "        ''' Calculates binary cross entropy loss on training set.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data.\n",
    "            reg_rate: regularization rate\n",
    "            \n",
    "        Returns:\n",
    "            loss.\n",
    "        '''\n",
    "        \n",
    "        predicted_prob = self.activation(X)\n",
    "        return (-1 * (np.sum(y * np.log(predicted_prob) + (1-y) *\n",
    "                            np.log(1-predicted_prob))) + \n",
    "                           reg_rate * np.dot(np.transpose(self.w), self.w))\n",
    "    \n",
    "    def calculate_gradient(self, x: np.ndarray, y: np.ndarray,\n",
    "                              reg_rate:float) -> np.ndarray:\n",
    "        '''Calculates gradients of loss function w.r.t. weight vector on training set.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data.\n",
    "            reg_rate: regularization rate\n",
    "        \n",
    "        Returns:\n",
    "            A vector of gradients.\n",
    "        '''\n",
    "        return np.transpose(X)@(self.activation(X) -y) + reg_rate * self.w\n",
    "    \n",
    "    def update_weight(self, grad:np.ndarray, lr:float) -> np.ndarray:\n",
    "        '''Updates the weights based on the gradient of loss function.\n",
    "        \n",
    "        Weight updates are carried out with the following formula:\n",
    "            w_new := w_old - lr* grad\n",
    "        \n",
    "        Args:\n",
    "            grad: gradient of loss w.r.t w\n",
    "            lr: learning rate\n",
    "        \n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "        return (self.w - lr*grad)\n",
    "    def gd(self, X:np.ndarray, y:np.ndarray,\n",
    "              num_epochs:int, lr:float, reg_rate:float) -> np.ndarray:\n",
    "        '''Estimates parameters of linear regression model through gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data.\n",
    "            num_epochs: Number of training steps\n",
    "            lr: Learning rate\n",
    "            reg_rate: regularization rate\n",
    "        \n",
    "        Return:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342cea4c",
   "metadata": {},
   "source": [
    "# Logistic Regression demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e06d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f1b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a toy data set creaing function\n",
    "\n",
    "def create_toy_data():\n",
    "    x0 = np.random.normal(size=50).reshape(-1,2) - 1\n",
    "    x1 = np.random.normal(size=50).reshape(-1,2) +1\n",
    "    return np.concatenate([x0,x1]), np.concatenate([np.zeros(shape=50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09345c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix:  (50, 2)\n",
      "sahpe of label vector:  (50,)\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, label_vector = create_toy_data()\n",
    "print('Shape of feature matrix: ', feature_matrix.shape)\n",
    "print('sahpe of label vector: ', label_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255756cd",
   "metadata": {},
   "source": [
    "## Polynomial transfromation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca1b72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ba109f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1a5ca8add897>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeature_matrix_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolynomial_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\apython\\JUPYTER_NOTEBOOK\\iitm\\ml_models.py\u001b[0m in \u001b[0;36mpolynomial_transform\u001b[1;34m(x, degree, logging)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mx_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Transpose the feature matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# populates 1s as first features for each example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "feature_matrix_bias = polynomial_transform(feature_matrix, degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15aed64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee13513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
